{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ecbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan 28 19:04:18 2023\n",
    "\n",
    "@author: gparu\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n",
    "data_file=\"MNIST_data.pkl\"\n",
    "# load the data set\n",
    "with open(data_file,'rb') as infile:\n",
    "    train_dataset = pickle.load(infile)\n",
    "X = train_dataset['X']\n",
    "y = train_dataset['y']\n",
    "\n",
    "# reshape the X data to (num_samples, height, width, channels)\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_val = X_val.astype(\"float32\") / 255\n",
    "\n",
    "# choose 16 random indices from the train set\n",
    "indices = random.sample(range(len(X_train)), 16)\n",
    "\n",
    "# select the corresponding images and labels\n",
    "images = X_train[indices]\n",
    "labels = y_train[indices]\n",
    "\n",
    "# create a 4x4 grid of subplots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# plot the images in the subplots\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(labels[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array.\n",
    "    Return:\n",
    "    relu func applied to each element of z\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    returns computed probabilitites for each element in batch separately\n",
    "    input: (N, 10)\n",
    "    output: (N, 10)\n",
    "    \"\"\"\n",
    "    # subtract the max for numerical stability\n",
    "    z -= np.max(z, axis=-1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "def init_params(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        input_size: size of input layer (784,)\n",
    "        hidden_size: size of hidden layer (integer)\n",
    "        output_size: size of output layer (10,)\n",
    "    Returns:\n",
    "        parameters: dictionary containing the weights and biases for each layer\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    W1 = np.random.normal(0, 0.01, (input_size, hidden_size))\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.normal(0, 0.01, (hidden_size, output_size))\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    return {0: W1, 1: b1, 2: W2, 3: b2}\n",
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle the training data\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    return X, y\n",
    "def forward_propg(X, parameters):\n",
    "    \"\"\"\n",
    "    X: input data\n",
    "    parameters: dictionary containing the weights and biases for each layer\n",
    "    returns: logits, output of each layer z1,z2,a1,a2\n",
    "    \"\"\"\n",
    "    W1 = parameters[0]\n",
    "    b1 = parameters[1]\n",
    "    W2 = parameters[2]\n",
    "    b2 = parameters[3]\n",
    "    X2=X.reshape(X.shape[0], -1)\n",
    "    # linear forward\n",
    "    z1 = np.dot(X2, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    global logits\n",
    "    logits=z2\n",
    "    cache = {0: z1, 1: a1, 2: z2, 3: logits}\n",
    "    return logits, cache\n",
    "def backward_propg(parameters, X, y, cache, learning_rate):\n",
    "    \"\"\"\n",
    "    parameters: dictionary containing the weights and biases for each layer\n",
    "    X: input data\n",
    "    y: true labels\n",
    "    cache: dictionary containing the output of the forward propagation step\n",
    "    learning_rate: step size for updating the weights\n",
    "    returns: updated parameters\n",
    "    \"\"\"\n",
    "    W2 = parameters[2]\n",
    "    X=X.reshape(48000, 784)\n",
    "    # backward prop\n",
    "    m = X.shape[0]\n",
    "    logits = softmax(cache[3])\n",
    "    a1 = cache[1]\n",
    "    \n",
    "    dz2 = (logits-y)\n",
    "    dw2 = (1/m) * np.dot(a1.T, dz2)\n",
    "    db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dw1 = (1/m) * np.dot(X.T, dz1)\n",
    "    db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    # update parameters\n",
    "    parameters[0] = parameters[0] - learning_rate * dw1\n",
    "    parameters[1] = parameters[1] - learning_rate * db1\n",
    "    parameters[2] = parameters[2] - learning_rate * dw2\n",
    "    parameters[3] = parameters[3] - learning_rate * db2\n",
    "    \n",
    "    return parameters\n",
    "def cost_func(parameters, X, y):\n",
    "    \"\"\"\n",
    "    parameters: dictionary containing the weights and biases for each layer\n",
    "    X: input data\n",
    "    y: true labels\n",
    "    returns: cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    logits2=softmax(parameters)\n",
    "    loss = -(1/m) * np.sum(y * np.log(logits2))\n",
    "    return loss\n",
    "def train(X, y, hidden_nodes, epochs=1000, lr=1e-5):\n",
    "    \"\"\"\n",
    "    hidden_nodes: no. of nodes in hidden layer\n",
    "    should return the updated optimize weights.\n",
    "    \"\"\"\n",
    "    # initialize weights\n",
    "    parameters = init_params(input_size=784, hidden_size=hidden_nodes, output_size=10)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # shuffle data\n",
    "        X, y = shuffle(X, y)\n",
    "        \n",
    "        # forward propagation\n",
    "        logits = forward_propg(X, parameters)[0]\n",
    "        cache = forward_propg(X, parameters)[1]\n",
    "        \n",
    "        # calculate cost\n",
    "        cost = cost_func(logits, X, y)\n",
    "        \n",
    "        # print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(\"Cost at iteration {}: {}\".format(i, cost))\n",
    "        \n",
    "        # backward propagation\n",
    "        parameters = backward_propg(parameters, X, y, cache, learning_rate=lr)\n",
    "    \n",
    "    return parameters\n",
    "def predict(X, updated_weights):\n",
    "    \"\"\"\n",
    "    returns the prediction in [0,9] for each element in X\n",
    "    \"\"\"\n",
    "    # perform forward propagation\n",
    "    logits = forward_propg(X, updated_weights)[0]\n",
    "    \n",
    "    # apply softmax to get probabilities\n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    # get the index of the highest probability for each sample\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return predictions\n",
    "def accuracy(predictions, y):\n",
    "    \"\"\"\n",
    "    prints % accuracy\n",
    "    \"\"\"\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "roll_num = \"22B1053\" # enter ldap\n",
    "hidden_dim = 256 # replace with your own hidden dimension\n",
    "\n",
    "# train the model\n",
    "updated_weights = train(X_train, y_train, hidden_dim)\n",
    "\n",
    "# make predictions\n",
    "predictions = predict(X_val, updated_weights)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy(predictions, y_val)\n",
    "\n",
    "# save the model\n",
    "model_dict = {\n",
    "    'z': hidden_dim, # hidden dimension of your model\n",
    "    'layer_0_wt': updated_weights[0], # layer 0 weight (784, z)\n",
    "    'layer_0_bias': updated_weights[1], # layer 0 bias (z, 1)\n",
    "    'layer_1_wt': updated_weights[2], # layer 1 weight (z, 10)\n",
    "    'layer_1_bias': updated_weights[3] # layer 1 bias (10, 1)\n",
    "}\n",
    "model_dict['layer_0_wt'].reshape(784, hidden_dim)\n",
    "model_dict['layer_0_bias'].reshape(hidden_dim, 1)\n",
    "model_dict['layer_1_wt'].reshape(hidden_dim, 10)\n",
    "model_dict['layer_1_bias'].reshape(10, 1)\n",
    "with open(f'model_{roll_num}.pkl', 'wb') as f:\n",
    "    pickle.dump(model_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44789c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3fd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
